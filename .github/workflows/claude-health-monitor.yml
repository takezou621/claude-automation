name: Claude Health Monitor & Service Quality Assurance

on:
  schedule:
    # Health checks every 15 minutes during business hours
    - cron: '*/15 6-18 * * 1-5'  # Every 15min, 6-18 UTC (15-03 JST), Mon-Fri
    # Deep health checks every hour during off-hours
    - cron: '0 * * * *'          # Every hour
  workflow_dispatch:
    inputs:
      health_check_type:
        description: 'Type of health check to perform'
        required: false
        default: 'full'
        type: choice
        options:
          - 'quick'
          - 'full'
          - 'deep'
          - 'emergency'
  push:
    paths:
      - '.github/workflows/**'
  workflow_run:
    workflows: ["Claude Full Automation", "Claude Ultimate Automation", "Claude Smart Automation"]
    types: [completed]

jobs:
  health-monitor:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      issues: write
      pull-requests: write
      actions: read
      checks: read
      
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ github.token }}
        
    - name: Claude Health Monitor Engine
      uses: actions/github-script@v7
      with:
        github-token: ${{ github.token }}
        script: |
          console.log('üè• CLAUDE HEALTH MONITOR ENGINE ACTIVATED');
          const startTime = Date.now();
          const executionId = Math.random().toString(36).substring(7);
          const timestamp = new Date().toISOString();
          
          // Health check type from input or default
          const healthCheckType = context.payload.inputs?.health_check_type || 'full';
          console.log(`üîç Health Check Type: ${healthCheckType}`);
          console.log(`‚ö° Execution ID: ${executionId}`);
          console.log(`üïí Timestamp: ${timestamp}`);
          
          // Initialize comprehensive health metrics
          const healthMetrics = {
            systemHealth: 'unknown',
            apiHealth: 'unknown',
            workflowHealth: 'unknown',
            performanceHealth: 'unknown',
            securityHealth: 'unknown',
            overallScore: 0,
            issues: [],
            warnings: [],
            recommendations: [],
            sloCompliance: {
              automationSuccessRate: 0,
              averageResponseTime: 0,
              errorRate: 0,
              availabilityScore: 0
            },
            startTime: startTime
          };
          
          try {
            console.log('\nüî¨ === PHASE 1: SYSTEM HEALTH ASSESSMENT ===');
            
            // Check GitHub API rate limits
            const rateLimitResponse = await github.rest.rateLimit.get();
            const rateLimit = rateLimitResponse.data.rate;
            const remaining = rateLimit.remaining;
            const total = rateLimit.limit;
            const resetTime = new Date(rateLimit.reset * 1000);
            
            console.log(`üìä API Rate Limit: ${remaining}/${total} (${Math.round(remaining/total*100)}%)`);
            console.log(`‚è∞ Reset Time: ${resetTime.toISOString()}`);
            
            // Rate limit health assessment
            if (remaining / total > 0.8) {
              healthMetrics.apiHealth = 'excellent';
            } else if (remaining / total > 0.5) {
              healthMetrics.apiHealth = 'good';
              healthMetrics.warnings.push('API rate limit at moderate usage');
            } else if (remaining / total > 0.2) {
              healthMetrics.apiHealth = 'degraded';
              healthMetrics.warnings.push('API rate limit approaching threshold');
            } else {
              healthMetrics.apiHealth = 'critical';
              healthMetrics.issues.push('API rate limit critically low');
            }
            
            console.log('\nüîç === PHASE 2: WORKFLOW HEALTH ANALYSIS ===');
            
            // Get recent workflow runs (last 50)
            const workflowRuns = await github.rest.actions.listWorkflowRunsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 50,
              created: `>${new Date(Date.now() - 24*60*60*1000).toISOString()}` // Last 24 hours
            });
            
            console.log(`üìà Workflow runs in last 24h: ${workflowRuns.data.total_count}`);
            
            // Analyze workflow success rates
            const runs = workflowRuns.data.workflow_runs;
            const totalRuns = runs.length;
            const successfulRuns = runs.filter(run => run.conclusion === 'success').length;
            const failedRuns = runs.filter(run => run.conclusion === 'failure').length;
            const cancelledRuns = runs.filter(run => run.conclusion === 'cancelled').length;
            
            const successRate = totalRuns > 0 ? (successfulRuns / totalRuns) * 100 : 100;
            const failureRate = totalRuns > 0 ? (failedRuns / totalRuns) * 100 : 0;
            
            console.log(`‚úÖ Success Rate: ${successRate.toFixed(2)}% (${successfulRuns}/${totalRuns})`);
            console.log(`‚ùå Failure Rate: ${failureRate.toFixed(2)}% (${failedRuns}/${totalRuns})`);
            console.log(`‚è∏Ô∏è Cancelled: ${cancelledRuns}`);
            
            healthMetrics.sloCompliance.automationSuccessRate = successRate;
            healthMetrics.sloCompliance.errorRate = failureRate;
            
            // Workflow health assessment
            if (successRate >= 99) {
              healthMetrics.workflowHealth = 'excellent';
            } else if (successRate >= 95) {
              healthMetrics.workflowHealth = 'good';
            } else if (successRate >= 90) {
              healthMetrics.workflowHealth = 'degraded';
              healthMetrics.warnings.push(`Workflow success rate below target: ${successRate.toFixed(2)}%`);
            } else {
              healthMetrics.workflowHealth = 'critical';
              healthMetrics.issues.push(`Critical workflow failure rate: ${failureRate.toFixed(2)}%`);
            }
            
            console.log('\n‚ö° === PHASE 3: PERFORMANCE ANALYSIS ===');
            
            // Analyze workflow execution times
            const executionTimes = runs
              .filter(run => run.run_started_at && run.updated_at)
              .map(run => {
                const start = new Date(run.run_started_at);
                const end = new Date(run.updated_at);
                return (end - start) / 1000; // seconds
              });
            
            if (executionTimes.length > 0) {
              const avgExecutionTime = executionTimes.reduce((a, b) => a + b, 0) / executionTimes.length;
              const maxExecutionTime = Math.max(...executionTimes);
              const minExecutionTime = Math.min(...executionTimes);
              
              console.log(`‚è±Ô∏è Avg Execution Time: ${avgExecutionTime.toFixed(2)}s`);
              console.log(`üî∫ Max Execution Time: ${maxExecutionTime.toFixed(2)}s`);
              console.log(`üîª Min Execution Time: ${minExecutionTime.toFixed(2)}s`);
              
              healthMetrics.sloCompliance.averageResponseTime = avgExecutionTime;
              
              // Performance health assessment
              if (avgExecutionTime <= 30) {
                healthMetrics.performanceHealth = 'excellent';
              } else if (avgExecutionTime <= 60) {
                healthMetrics.performanceHealth = 'good';
              } else if (avgExecutionTime <= 120) {
                healthMetrics.performanceHealth = 'degraded';
                healthMetrics.warnings.push(`Average execution time elevated: ${avgExecutionTime.toFixed(2)}s`);
              } else {
                healthMetrics.performanceHealth = 'critical';
                healthMetrics.issues.push(`Critical performance degradation: ${avgExecutionTime.toFixed(2)}s avg`);
              }
            } else {
              healthMetrics.performanceHealth = 'unknown';
              healthMetrics.warnings.push('No recent workflow data for performance analysis');
            }
            
            console.log('\nüõ°Ô∏è === PHASE 4: SECURITY & QUALITY ASSESSMENT ===');
            
            // Check for security-related labels and issues
            const securityIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'security,vulnerability,high-risk',
              state: 'open',
              per_page: 100
            });
            
            const openSecurityIssues = securityIssues.data.length;
            console.log(`üîí Open Security Issues: ${openSecurityIssues}`);
            
            // Security health assessment
            if (openSecurityIssues === 0) {
              healthMetrics.securityHealth = 'excellent';
            } else if (openSecurityIssues <= 2) {
              healthMetrics.securityHealth = 'good';
              healthMetrics.warnings.push(`${openSecurityIssues} open security issues detected`);
            } else if (openSecurityIssues <= 5) {
              healthMetrics.securityHealth = 'degraded';
              healthMetrics.warnings.push(`Multiple security issues require attention: ${openSecurityIssues}`);
            } else {
              healthMetrics.securityHealth = 'critical';
              healthMetrics.issues.push(`Critical security issues count: ${openSecurityIssues}`);
            }
            
            console.log('\nüìä === PHASE 5: SLO COMPLIANCE EVALUATION ===');
            
            // Calculate overall SLO compliance
            const sloTargets = {
              automationSuccessRate: 99.9,
              maxAverageResponseTime: 30,
              maxErrorRate: 0.1,
              minAvailabilityScore: 99.5
            };
            
            const sloCompliance = {
              automationSuccess: healthMetrics.sloCompliance.automationSuccessRate >= sloTargets.automationSuccessRate,
              responseTime: healthMetrics.sloCompliance.averageResponseTime <= sloTargets.maxAverageResponseTime,
              errorRate: healthMetrics.sloCompliance.errorRate <= sloTargets.maxErrorRate,
              availability: true // Simplified for now
            };
            
            const sloComplianceCount = Object.values(sloCompliance).filter(Boolean).length;
            const sloCompliancePercentage = (sloComplianceCount / Object.keys(sloCompliance).length) * 100;
            
            console.log(`üéØ SLO Compliance: ${sloCompliancePercentage.toFixed(1)}% (${sloComplianceCount}/4)`);
            healthMetrics.sloCompliance.availabilityScore = sloCompliancePercentage;
            
            // Calculate overall health score
            const healthScores = {
              excellent: 100,
              good: 80,
              degraded: 60,
              critical: 20,
              unknown: 50
            };
            
            const scores = [
              healthScores[healthMetrics.apiHealth],
              healthScores[healthMetrics.workflowHealth],
              healthScores[healthMetrics.performanceHealth],
              healthScores[healthMetrics.securityHealth]
            ];
            
            healthMetrics.overallScore = scores.reduce((a, b) => a + b, 0) / scores.length;
            
            if (healthMetrics.overallScore >= 95) {
              healthMetrics.systemHealth = 'excellent';
            } else if (healthMetrics.overallScore >= 80) {
              healthMetrics.systemHealth = 'good';
            } else if (healthMetrics.overallScore >= 60) {
              healthMetrics.systemHealth = 'degraded';
            } else {
              healthMetrics.systemHealth = 'critical';
            }
            
            console.log('\nüè• === PHASE 6: HEALTH REPORT GENERATION ===');
            
            const executionTime = Math.round((Date.now() - startTime) / 1000);
            
            // Generate comprehensive health report
            const healthReport = `## üè• Claude Health Monitor Report
            
### üìä System Health Overview
**Overall Health**: ${getHealthEmoji(healthMetrics.systemHealth)} **${healthMetrics.systemHealth.toUpperCase()}** (Score: ${healthMetrics.overallScore.toFixed(1)}/100)
**Execution ID**: \`${executionId}\`
**Check Type**: ${healthCheckType}
**Execution Time**: ${executionTime}s

### üîç Component Health Status
| Component | Status | Score |
|-----------|--------|-------|
| üîå API Health | ${getHealthEmoji(healthMetrics.apiHealth)} ${healthMetrics.apiHealth} | ${healthScores[healthMetrics.apiHealth]}/100 |
| ‚öôÔ∏è Workflow Health | ${getHealthEmoji(healthMetrics.workflowHealth)} ${healthMetrics.workflowHealth} | ${healthScores[healthMetrics.workflowHealth]}/100 |
| ‚ö° Performance | ${getHealthEmoji(healthMetrics.performanceHealth)} ${healthMetrics.performanceHealth} | ${healthScores[healthMetrics.performanceHealth]}/100 |
| üõ°Ô∏è Security | ${getHealthEmoji(healthMetrics.securityHealth)} ${healthMetrics.securityHealth} | ${healthScores[healthMetrics.securityHealth]}/100 |

### üìà SLO Compliance Metrics
- **Automation Success Rate**: ${healthMetrics.sloCompliance.automationSuccessRate.toFixed(2)}% (Target: ‚â•99.9%)
- **Average Response Time**: ${healthMetrics.sloCompliance.averageResponseTime.toFixed(2)}s (Target: ‚â§30s)
- **Error Rate**: ${healthMetrics.sloCompliance.errorRate.toFixed(2)}% (Target: ‚â§0.1%)
- **Overall SLO Compliance**: ${healthMetrics.sloCompliance.availabilityScore.toFixed(1)}%

### üìä Performance Metrics
- **API Rate Limit**: ${remaining}/${total} (${Math.round(remaining/total*100)}%)
- **Workflow Runs (24h)**: ${totalRuns}
- **Success Rate**: ${successRate.toFixed(2)}%
- **Average Execution**: ${healthMetrics.sloCompliance.averageResponseTime.toFixed(2)}s

${healthMetrics.issues.length > 0 ? `### ‚ùå Critical Issues
${healthMetrics.issues.map(issue => `- üî¥ ${issue}`).join('\n')}
` : ''}${healthMetrics.warnings.length > 0 ? `### ‚ö†Ô∏è Warnings
${healthMetrics.warnings.map(warning => `- üü° ${warning}`).join('\n')}
` : ''}### üí° Recommendations
${healthMetrics.recommendations.length > 0 ? healthMetrics.recommendations.map(rec => `- üí° ${rec}`).join('\n') : '- ‚úÖ System operating within optimal parameters'}

### üéØ Next Health Check
**Schedule**: ${getNextHealthCheckTime()}
**Type**: Automatic full health assessment

---
üè• **Claude Health Monitor** | **High Service Quality**
‚ö° **99.9% SLO Target** | **Real-time Monitoring** | **Proactive Alerting**`;

            console.log(`üìã Health report generated (${healthReport.length} chars)`);
            
            // Create or update health monitoring issue
            const healthIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'health-monitor,system-status',
              state: 'open',
              per_page: 1
            });
            
            if (healthIssues.data.length > 0) {
              // Update existing health issue
              const healthIssue = healthIssues.data[0];
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: healthIssue.number,
                body: healthReport
              });
              console.log(`üìù Updated health issue #${healthIssue.number}`);
            } else {
              // Create new health monitoring issue
              const newHealthIssue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `üè• Claude System Health Monitor - ${timestamp}`,
                body: healthReport,
                labels: ['health-monitor', 'system-status', 'automation']
              });
              console.log(`üìã Created health issue #${newHealthIssue.data.number}`);
            }
            
            // Alert for critical issues
            if (healthMetrics.systemHealth === 'critical' || healthMetrics.issues.length > 0) {
              const alertIssue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `üö® CRITICAL: System Health Alert - ${timestamp}`,
                body: `## üö® CRITICAL SYSTEM HEALTH ALERT

**System Status**: ${getHealthEmoji(healthMetrics.systemHealth)} **${healthMetrics.systemHealth.toUpperCase()}**
**Alert Level**: CRITICAL
**Execution ID**: \`${executionId}\`

### Critical Issues Detected:
${healthMetrics.issues.map(issue => `- üî¥ ${issue}`).join('\n')}

### Immediate Actions Required:
- Review and address critical issues immediately
- Check system logs for additional context
- Consider enabling emergency protocols
- Monitor system closely for further degradation

### Contact Information:
- System Administrator: Immediate attention required
- DevOps Team: Emergency response activated

---
üö® **CRITICAL ALERT** | **Immediate Response Required**`,
                labels: ['critical-alert', 'health-monitor', 'urgent', 'system-failure']
              });
              console.log(`üö® CRITICAL ALERT ISSUED: #${alertIssue.data.number}`);
            }
            
            console.log(`\n‚úÖ CLAUDE HEALTH MONITOR COMPLETED`);
            console.log(`üè• Overall Health: ${healthMetrics.systemHealth} (${healthMetrics.overallScore.toFixed(1)}/100)`);
            console.log(`üìä SLO Compliance: ${healthMetrics.sloCompliance.availabilityScore.toFixed(1)}%`);
            console.log(`‚ö° Execution Time: ${executionTime}s`);
            
          } catch (error) {
            console.log(`‚ùå Health Monitor Error: ${error.message}`);
            
            // Create error alert
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `‚ùå Health Monitor Error - ${timestamp}`,
              body: `## ‚ùå Health Monitor System Error

The Claude Health Monitor encountered a critical error and could not complete the health assessment.

**Error Details:**
- **Message**: ${error.message}
- **Execution ID**: \`${executionId}\`
- **Timestamp**: ${timestamp}
- **Check Type**: ${healthCheckType}

**Immediate Actions:**
1. Review health monitor workflow configuration
2. Check GitHub API accessibility and permissions
3. Verify system resources and dependencies
4. Consider manual health assessment

**Error Context:**
\`\`\`
${error.stack}
\`\`\`

---
‚ùå **Health Monitor System Failure** | **Manual Intervention Required**`,
              labels: ['health-monitor-error', 'system-failure', 'urgent']
            });
            
            throw error;
          }
          
          // Helper function for health emojis
          function getHealthEmoji(health) {
            const emojis = {
              excellent: 'üü¢',
              good: 'üü°',
              degraded: 'üü†',
              critical: 'üî¥',
              unknown: '‚ö™'
            };
            return emojis[health] || '‚ö™';
          }
          
          // Helper function for next health check time
          function getNextHealthCheckTime() {
            const now = new Date();
            const next = new Date(now.getTime() + 15 * 60 * 1000); // +15 minutes
            return next.toISOString();
          }