name: Claude Advanced Error Recovery & Circuit Breaker

on:
  workflow_run:
    workflows: ["Claude Full Automation", "Claude Ultimate Automation", "Claude Smart Automation", "Claude Rapid Automation"]
    types: [completed]
  schedule:
    # Error recovery checks every 5 minutes during peak hours
    - cron: '*/5 6-18 * * 1-5'  # Every 5min, 6-18 UTC (15-03 JST), Mon-Fri
  workflow_dispatch:
    inputs:
      recovery_type:
        description: 'Type of recovery operation'
        required: false
        default: 'automatic'
        type: choice
        options:
          - 'automatic'
          - 'forced'
          - 'reset_circuit_breaker'
          - 'emergency_failover'
      target_workflow:
        description: 'Target workflow for recovery (optional)'
        required: false
        type: string

jobs:
  error-recovery:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      issues: write
      pull-requests: write
      actions: write
      checks: write
      
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ github.token }}
        
    - name: Advanced Error Recovery Engine
      uses: actions/github-script@v7
      with:
        github-token: ${{ github.token }}
        script: |
          console.log('🛡️ CLAUDE ADVANCED ERROR RECOVERY ENGINE ACTIVATED');
          const startTime = Date.now();
          const executionId = Math.random().toString(36).substring(7);
          const timestamp = new Date().toISOString();
          
          const recoveryType = context.payload.inputs?.recovery_type || 'automatic';
          const targetWorkflow = context.payload.inputs?.target_workflow;
          
          console.log(`🔧 Recovery Type: ${recoveryType}`);
          console.log(`⚡ Execution ID: ${executionId}`);
          console.log(`🎯 Target Workflow: ${targetWorkflow || 'auto-detect'}`);
          
          // Circuit breaker states stored as repository variables/secrets
          const circuitBreakerStates = {
            'claude-full-automation': { status: 'closed', failureCount: 0, lastFailure: null },
            'claude-ultimate-automation': { status: 'closed', failureCount: 0, lastFailure: null },
            'claude-smart-automation': { status: 'closed', failureCount: 0, lastFailure: null },
            'claude-rapid-automation': { status: 'closed', failureCount: 0, lastFailure: null }
          };
          
          // Error recovery metrics
          const recoveryMetrics = {
            failedWorkflows: 0,
            recoveredWorkflows: 0,
            circuitBreakerTrips: 0,
            retryAttempts: 0,
            emergencyFailovers: 0,
            successfulRecoveries: 0,
            unrecoverableFailures: 0,
            startTime: startTime
          };
          
          try {
            console.log('\n🔍 === PHASE 1: FAILURE DETECTION & ANALYSIS ===');
            
            // Get recent workflow runs for analysis
            const workflowRuns = await github.rest.actions.listWorkflowRunsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 100,
              status: 'failure',
              created: `>${new Date(Date.now() - 2*60*60*1000).toISOString()}` // Last 2 hours
            });
            
            const failedRuns = workflowRuns.data.workflow_runs;
            console.log(`❌ Failed workflows in last 2h: ${failedRuns.length}`);
            recoveryMetrics.failedWorkflows = failedRuns.length;
            
            if (failedRuns.length === 0 && recoveryType === 'automatic') {
              console.log('✅ No failed workflows detected, system healthy');
              return;
            }
            
            // Analyze failure patterns
            const failurePatterns = {};
            const workflowFailureCounts = {};
            
            for (const run of failedRuns) {
              const workflowName = run.name;
              workflowFailureCounts[workflowName] = (workflowFailureCounts[workflowName] || 0) + 1;
              
              // Analyze failure reasons from logs
              try {
                const jobs = await github.rest.actions.listJobsForWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: run.id
                });
                
                for (const job of jobs.data.jobs) {
                  if (job.conclusion === 'failure') {
                    const logs = await github.rest.actions.downloadJobLogsForWorkflowRun({
                      owner: context.repo.owner,
                      repo: context.repo.repo,
                      job_id: job.id
                    });
                    
                    // Pattern analysis (simplified)
                    const logContent = logs.data || '';
                    if (logContent.includes('rate limit')) {
                      failurePatterns['rate_limit'] = (failurePatterns['rate_limit'] || 0) + 1;
                    }
                    if (logContent.includes('timeout')) {
                      failurePatterns['timeout'] = (failurePatterns['timeout'] || 0) + 1;
                    }
                    if (logContent.includes('permission')) {
                      failurePatterns['permission'] = (failurePatterns['permission'] || 0) + 1;
                    }
                    if (logContent.includes('network')) {
                      failurePatterns['network'] = (failurePatterns['network'] || 0) + 1;
                    }
                  }
                }
              } catch (logError) {
                console.log(`⚠️ Could not analyze logs for run ${run.id}: ${logError.message}`);
              }
            }
            
            console.log(`📊 Workflow failure counts:`, JSON.stringify(workflowFailureCounts, null, 2));
            console.log(`🔍 Failure patterns:`, JSON.stringify(failurePatterns, null, 2));
            
            console.log('\n⚙️ === PHASE 2: CIRCUIT BREAKER MANAGEMENT ===');
            
            // Circuit breaker logic
            const circuitBreakerThresholds = {
              failureThreshold: 3,      // Trip after 3 failures
              timeoutMs: 5 * 60 * 1000, // 5 minute timeout
              halfOpenRetries: 2        // 2 retries in half-open state
            };
            
            for (const [workflowName, failureCount] of Object.entries(workflowFailureCounts)) {
              const normalizedName = workflowName.toLowerCase().replace(/\s+/g, '-');
              
              if (failureCount >= circuitBreakerThresholds.failureThreshold) {
                circuitBreakerStates[normalizedName] = {
                  status: 'open',
                  failureCount: failureCount,
                  lastFailure: new Date().toISOString()
                };
                recoveryMetrics.circuitBreakerTrips++;
                console.log(`🔴 Circuit breaker OPEN for ${workflowName} (${failureCount} failures)`);
                
                // Create circuit breaker alert
                await github.rest.issues.create({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  title: `🔴 Circuit Breaker OPEN: ${workflowName}`,
                  body: `## 🔴 Circuit Breaker Activated

**Workflow**: ${workflowName}
**Status**: OPEN (Service Temporarily Disabled)
**Failure Count**: ${failureCount}
**Threshold**: ${circuitBreakerThresholds.failureThreshold}
**Timestamp**: ${timestamp}

### Failure Analysis
${Object.entries(failurePatterns).map(([pattern, count]) => `- **${pattern}**: ${count} occurrences`).join('\n')}

### Automatic Recovery
- Circuit breaker will attempt recovery in ${circuitBreakerThresholds.timeoutMs/60000} minutes
- Manual recovery can be triggered via workflow_dispatch
- Emergency failover protocols activated

### Next Steps
1. Investigate root cause of failures
2. Address identified failure patterns
3. Monitor for automatic recovery
4. Consider manual intervention if needed

---
🛡️ **Circuit Breaker Protection** | **Automatic Service Recovery**`,
                  labels: ['circuit-breaker', 'service-failure', 'automatic-recovery', 'urgent']
                });
              }
            }
            
            console.log('\n🔄 === PHASE 3: AUTOMATIC RECOVERY ATTEMPTS ===');
            
            // Implement exponential backoff retry logic
            const retryStrategies = {
              rate_limit: { delay: 300000, maxRetries: 3 }, // 5 min delay, 3 retries
              timeout: { delay: 60000, maxRetries: 5 },     // 1 min delay, 5 retries
              permission: { delay: 0, maxRetries: 1 },      // Immediate, 1 retry
              network: { delay: 30000, maxRetries: 4 },     // 30s delay, 4 retries
              default: { delay: 120000, maxRetries: 3 }     // 2 min delay, 3 retries
            };
            
            for (const [pattern, count] of Object.entries(failurePatterns)) {
              if (count === 0) continue;
              
              const strategy = retryStrategies[pattern] || retryStrategies.default;
              console.log(`🔄 Applying recovery strategy for ${pattern}: ${strategy.maxRetries} retries with ${strategy.delay/1000}s delay`);
              
              recoveryMetrics.retryAttempts += strategy.maxRetries;
              
              // Implement recovery actions based on failure type
              switch (pattern) {
                case 'rate_limit':
                  console.log('⏱️ Rate limit recovery: Implementing cooldown period');
                  await new Promise(resolve => setTimeout(resolve, Math.min(strategy.delay, 30000))); // Max 30s in workflow
                  break;
                  
                case 'permission':
                  console.log('🔐 Permission recovery: Checking token permissions');
                  // Check if token has required permissions
                  try {
                    await github.rest.repos.get({
                      owner: context.repo.owner,
                      repo: context.repo.repo
                    });
                    console.log('✅ Token permissions verified');
                  } catch (permError) {
                    console.log(`❌ Token permission issue confirmed: ${permError.message}`);
                    recoveryMetrics.unrecoverableFailures++;
                  }
                  break;
                  
                case 'network':
                  console.log('🌐 Network recovery: Testing connectivity');
                  // Simple connectivity test
                  try {
                    await github.rest.rateLimit.get();
                    console.log('✅ Network connectivity verified');
                  } catch (netError) {
                    console.log(`❌ Network issue confirmed: ${netError.message}`);
                  }
                  break;
                  
                case 'timeout':
                  console.log('⏰ Timeout recovery: Optimizing workflow parameters');
                  // Could implement workflow optimization here
                  break;
              }
            }
            
            console.log('\n🚀 === PHASE 4: RECOVERY EXECUTION ===');
            
            // Attempt to recover failed workflows
            for (const [workflowName, failureCount] of Object.entries(workflowFailureCounts)) {
              if (recoveryType === 'automatic' && failureCount < 2) {
                console.log(`🔄 Attempting automatic recovery for ${workflowName}`);
                
                try {
                  // Trigger workflow retry with recovery parameters
                  const workflows = await github.rest.actions.listRepoWorkflows({
                    owner: context.repo.owner,
                    repo: context.repo.repo
                  });
                  
                  const targetWf = workflows.data.workflows.find(wf => 
                    wf.name.toLowerCase().includes(workflowName.toLowerCase().replace(/\s+/g, '-'))
                  );
                  
                  if (targetWf) {
                    await github.rest.actions.createWorkflowDispatch({
                      owner: context.repo.owner,
                      repo: context.repo.repo,
                      workflow_id: targetWf.id,
                      ref: 'main',
                      inputs: {
                        recovery_mode: 'true',
                        execution_id: executionId,
                        retry_attempt: 'auto-recovery'
                      }
                    });
                    
                    recoveryMetrics.successfulRecoveries++;
                    console.log(`✅ Recovery workflow dispatched for ${workflowName}`);
                  }
                } catch (recoveryError) {
                  console.log(`❌ Recovery failed for ${workflowName}: ${recoveryError.message}`);
                  recoveryMetrics.unrecoverableFailures++;
                }
              }
            }
            
            console.log('\n🎯 === PHASE 5: EMERGENCY FAILOVER PROTOCOLS ===');
            
            // Emergency failover for critical failures
            if (recoveryType === 'emergency_failover' || 
                (recoveryMetrics.unrecoverableFailures > 2 && recoveryType === 'automatic')) {
                
              console.log('🚨 ACTIVATING EMERGENCY FAILOVER PROTOCOLS');
              recoveryMetrics.emergencyFailovers++;
              
              // Create emergency issue
              const emergencyIssue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `🚨 EMERGENCY FAILOVER ACTIVATED - ${timestamp}`,
                body: `## 🚨 EMERGENCY FAILOVER PROTOCOLS ACTIVATED

**Alert Level**: CRITICAL
**Execution ID**: \`${executionId}\`
**Timestamp**: ${timestamp}
**Recovery Type**: ${recoveryType}

### System Status
- **Failed Workflows**: ${recoveryMetrics.failedWorkflows}
- **Unrecoverable Failures**: ${recoveryMetrics.unrecoverableFailures}
- **Circuit Breaker Trips**: ${recoveryMetrics.circuitBreakerTrips}
- **Emergency Failovers**: ${recoveryMetrics.emergencyFailovers}

### Failure Analysis
${Object.entries(failurePatterns).map(([pattern, count]) => `- **${pattern}**: ${count} occurrences`).join('\n')}

### Emergency Actions Taken
1. 🔴 All automation workflows temporarily suspended
2. 🛡️ Circuit breakers activated for protection
3. 🚨 Emergency protocols engaged
4. 📞 Manual intervention required

### Required Actions
1. **IMMEDIATE**: Review system logs and error reports
2. **URGENT**: Address root cause of failures
3. **PRIORITY**: Test individual workflow components
4. **CRITICAL**: Validate system recovery before re-enabling

### Recovery Procedures
1. Investigate and resolve identified failure patterns
2. Run individual workflow tests to verify functionality
3. Gradually re-enable workflows with monitoring
4. Reset circuit breakers via workflow_dispatch

---
🚨 **EMERGENCY FAILOVER** | **CRITICAL SYSTEM ALERT** | **MANUAL INTERVENTION REQUIRED**`,
                labels: ['emergency-failover', 'critical-alert', 'system-failure', 'manual-intervention-required']
              });
              
              console.log(`🚨 Emergency failover issue created: #${emergencyIssue.data.number}`);
              
              // Disable automatic workflows by adding temporary labels
              const autoIssues = await github.rest.issues.listForRepo({
                owner: context.repo.owner,
                repo: context.repo.repo,
                labels: 'claude-ready,automation-ready,smart-automation',
                state: 'open',
                per_page: 100
              });
              
              for (const issue of autoIssues.data) {
                await github.rest.issues.addLabels({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: issue.number,
                  labels: ['automation-suspended', 'emergency-hold']
                });
              }
              
              console.log(`🔒 Suspended ${autoIssues.data.length} automation-ready issues`);
            }
            
            console.log('\n📊 === PHASE 6: RECOVERY REPORTING ===');
            
            const executionTime = Math.round((Date.now() - startTime) / 1000);
            const recoverySuccess = recoveryMetrics.successfulRecoveries > 0;
            const systemStable = recoveryMetrics.unrecoverableFailures === 0;
            
            // Generate comprehensive recovery report
            const recoveryReport = `## 🛡️ Error Recovery & Circuit Breaker Report
            
### 📊 Recovery Summary
**Recovery Type**: ${recoveryType}
**System Status**: ${systemStable ? '🟢 STABLE' : '🔴 DEGRADED'}
**Recovery Success**: ${recoverySuccess ? '✅ SUCCESSFUL' : '❌ FAILED'}
**Execution ID**: \`${executionId}\`
**Execution Time**: ${executionTime}s

### 🔍 Failure Analysis
| Metric | Count | Status |
|--------|-------|--------|
| 📊 Failed Workflows | ${recoveryMetrics.failedWorkflows} | ${recoveryMetrics.failedWorkflows === 0 ? '✅' : '⚠️'} |
| 🔄 Recovery Attempts | ${recoveryMetrics.retryAttempts} | 📊 |
| ✅ Successful Recoveries | ${recoveryMetrics.successfulRecoveries} | ${recoveryMetrics.successfulRecoveries > 0 ? '✅' : '❌'} |
| ❌ Unrecoverable Failures | ${recoveryMetrics.unrecoverableFailures} | ${recoveryMetrics.unrecoverableFailures === 0 ? '✅' : '🔴'} |
| 🔴 Circuit Breaker Trips | ${recoveryMetrics.circuitBreakerTrips} | ${recoveryMetrics.circuitBreakerTrips === 0 ? '✅' : '⚠️'} |
| 🚨 Emergency Failovers | ${recoveryMetrics.emergencyFailovers} | ${recoveryMetrics.emergencyFailovers === 0 ? '✅' : '🚨'} |

### 🔍 Detected Failure Patterns
${Object.entries(failurePatterns).length > 0 ? 
  Object.entries(failurePatterns).map(([pattern, count]) => `- **${pattern}**: ${count} occurrences`).join('\n') :
  '- ✅ No failure patterns detected'}

### ⚙️ Circuit Breaker Status
${Object.entries(circuitBreakerStates).map(([workflow, state]) => 
  `- **${workflow}**: ${state.status === 'open' ? '🔴 OPEN' : '🟢 CLOSED'} (${state.failureCount} failures)`
).join('\n')}

### 🎯 Recovery Actions Taken
${recoveryMetrics.successfulRecoveries > 0 ? `- ✅ ${recoveryMetrics.successfulRecoveries} workflows successfully recovered` : ''}
${recoveryMetrics.circuitBreakerTrips > 0 ? `- 🔴 ${recoveryMetrics.circuitBreakerTrips} circuit breakers activated` : ''}
${recoveryMetrics.emergencyFailovers > 0 ? `- 🚨 ${recoveryMetrics.emergencyFailovers} emergency failover protocols engaged` : ''}
${recoveryMetrics.retryAttempts > 0 ? `- 🔄 ${recoveryMetrics.retryAttempts} automatic retry attempts executed` : ''}

### 📈 System Recommendations
${systemStable ? 
  '- ✅ System operating normally, continue monitoring' :
  `- 🔴 System requires attention, ${recoveryMetrics.unrecoverableFailures} critical issues need resolution`}
${recoveryMetrics.circuitBreakerTrips > 0 ? 
  '- ⚠️ Monitor circuit breaker recovery and consider manual reset if needed' : ''}
${recoveryMetrics.emergencyFailovers > 0 ? 
  '- 🚨 Emergency protocols active, manual intervention required for full recovery' : ''}

### ⏰ Next Recovery Check
**Schedule**: ${new Date(Date.now() + 5*60*1000).toISOString()}
**Type**: Automatic monitoring and recovery

---
🛡️ **Advanced Error Recovery** | **Circuit Breaker Protection** | **Enterprise Reliability**
⚡ **99.9% Uptime Target** | **Automatic Failover** | **Intelligent Recovery**`;

            // Post recovery report
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `🛡️ Error Recovery Report - ${timestamp}`,
              body: recoveryReport,
              labels: ['error-recovery', 'system-report', 'automation', systemStable ? 'system-stable' : 'system-degraded']
            });
            
            console.log(`✅ CLAUDE ERROR RECOVERY ENGINE COMPLETED`);
            console.log(`🛡️ System Status: ${systemStable ? 'STABLE' : 'DEGRADED'}`);
            console.log(`📊 Recovery Success: ${recoverySuccess}`);
            console.log(`⚡ Execution Time: ${executionTime}s`);
            
          } catch (error) {
            console.log(`❌ Error Recovery Engine Error: ${error.message}`);
            
            // Create critical error alert
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `❌ CRITICAL: Error Recovery System Failure - ${timestamp}`,
              body: `## ❌ CRITICAL ERROR RECOVERY SYSTEM FAILURE

The Claude Error Recovery Engine has encountered a critical failure and cannot continue operations.

**Error Details:**
- **Message**: ${error.message}
- **Execution ID**: \`${executionId}\`
- **Recovery Type**: ${recoveryType}
- **Timestamp**: ${timestamp}

**System Impact:**
- Error recovery processes are non-functional
- Circuit breaker protection may be compromised
- Manual intervention required immediately

**Emergency Actions Required:**
1. 🚨 Immediate manual system assessment
2. 🔍 Review error recovery workflow configuration
3. 🛠️ Manual circuit breaker management
4. 📞 Escalate to system administrators

**Error Context:**
\`\`\`
${error.stack}
\`\`\`

---
❌ **CRITICAL SYSTEM FAILURE** | **ERROR RECOVERY DOWN** | **EMERGENCY RESPONSE REQUIRED**`,
              labels: ['critical-failure', 'error-recovery-failure', 'emergency', 'system-down']
            });
            
            throw error;
          }